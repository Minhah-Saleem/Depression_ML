{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6268e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "\n",
      "Positive training responses\n",
      "chunk: 1 \t True\n",
      "chunk: 2 \t True\n",
      "chunk: 3 \t True\n",
      "chunk: 4 \t True\n",
      "chunk: 5 \t True\n",
      "chunk: 6 \t True\n",
      "chunk: 7 \t True\n",
      "chunk: 8 \t True\n",
      "chunk: 9 \t True\n",
      "chunk: 10 \t True\n",
      "403\n",
      "\n",
      "Negative training responses\n",
      "chunk: 1 \t True\n",
      "chunk: 2 \t True\n",
      "chunk: 3 \t True\n",
      "chunk: 4 \t True\n",
      "chunk: 5 \t True\n",
      "chunk: 6 \t True\n",
      "chunk: 7 \t True\n",
      "chunk: 8 \t True\n",
      "chunk: 9 \t True\n",
      "chunk: 10 \t True\n",
      "401\n",
      "\n",
      "Test training responses\n",
      "chunk: 1 \t True\n",
      "chunk: 2 \t True\n",
      "chunk: 3 \t True\n",
      "chunk: 4 \t True\n",
      "chunk: 5 \t True\n",
      "chunk: 6 \t True\n",
      "chunk: 7 \t True\n",
      "chunk: 8 \t True\n",
      "chunk: 9 \t True\n",
      "chunk: 10 \t True\n",
      "Corpus Processing\n",
      "Corpus Features:\n",
      "Reddit user posts\n",
      "Separated into 10 chunks\n",
      "XML Format\n",
      "Training Data\n"
     ]
    }
   ],
   "source": [
    "# Verify if there is a match with the original XML files\n",
    "import os\n",
    "path_data = \"C:/Users/USER/eRisk2017/2017/\"\n",
    "# Get the list of the first chunk and check if it is present in the rest\n",
    "individuals_train_p = list(os.listdir(path_data+\"/train/positive_examples_anonymous_chunks/chunk 1\"))\n",
    "individuals_train_p = [individual.replace('1.xml','') for individual in individuals_train_p if individual !='desktop.ini']\n",
    "print(len(individuals_train_p))\n",
    "print('\\nPositive training responses')\n",
    "for chunk in range(1, 11):\n",
    "    individuals_in_chunk_p = os.listdir(path_data+'/train/positive_examples_anonymous_chunks/chunk '+str(chunk))\n",
    "    individuals_in_chunk_p = [individual.replace(str(chunk)+'.xml','') for individual in individuals_in_chunk_p if individual !='desktop.ini']\n",
    "    print('chunk:', chunk, '\\t', set(individuals_train_p) == set(individuals_in_chunk_p))\n",
    "individuals_train_n = list(os.listdir(path_data+\"/train/negative_examples_anonymous_chunks/chunk 1\"))\n",
    "individuals_train_n = [individual.replace('1.xml','') for individual in individuals_train_n if individual !='desktop.ini']\n",
    "print(len(individuals_train_n))\n",
    "print('\\nNegative training responses')\n",
    "for chunk in range(1, 11):\n",
    "    individuals_in_chunk_n = os.listdir(path_data+'/train/negative_examples_anonymous_chunks/chunk '+str(chunk))\n",
    "    individuals_in_chunk_n = [individual.replace(str(chunk)+'.xml','') for individual in individuals_in_chunk_n if individual !='desktop.ini']\n",
    "    print('chunk:', chunk, '\\t', set(individuals_train_n) == set(individuals_in_chunk_n))\n",
    "individuals_train_t = list(os.listdir(path_data+'/test/chunk 1/chunk 1/'))\n",
    "individuals_train_t = [individual.replace('1.xml','') for individual in individuals_train_t if individual !='desktop.ini']\n",
    "print(len(individuals_train_t))\n",
    "print('\\nTest training responses')\n",
    "for chunk in range(1, 11):\n",
    "    individuals_in_chunk_t = os.listdir(path_data+'/test/chunk '+str(chunk)+'/chunk '+str(chunk))\n",
    "    individuals_in_chunk_t = [individual.replace(str(chunk)+'.xml','') for individual in individuals_in_chunk_t if individual !='desktop.ini']\n",
    "    print('chunk:', chunk, '\\t', set(individuals_train_t) == set(individuals_in_chunk_t))\n",
    "\n",
    "# Corpus Processing\n",
    "print(\"Corpus Processing\")\n",
    "print(\"Corpus Features:\")\n",
    "print(\"Reddit user posts\")\n",
    "print(\"Separated into 10 chunks\")\n",
    "print(\"XML Format\")\n",
    "print(\"Training Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3c3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the folder had different no. of files. to debug :\n",
    "# individuals_train_n = list(os.listdir(path_data+\"/train/negative_examples_anonymous_chunks/chunk 1\"))\n",
    "# individuals_train_n = [individual.replace('1.xml','') for individual in individuals_train_n if individual !='desktop.ini']\n",
    "# print(len(individuals_train_n))\n",
    "# print('\\nNegative training responses')\n",
    "# for chunk in range(1, 11):\n",
    "#     individuals_in_chunk_n = os.listdir(path_data+'/train/negative_examples_anonymous_chunks/chunk '+str(chunk))\n",
    "#     individuals_in_chunk_n = [individual.replace(str(chunk)+'.xml','') for individual in individuals_in_chunk_n if individual !='desktop.ini']\n",
    "#     print('chunk:', chunk, '\\t', len(set(individuals_train_n)), len(set(individuals_in_chunk_n)))\n",
    "# #     print([i for i in set(individuals_train_n) if i not in set(individuals_in_chunk_n)])\n",
    "#     print([i for i in set(individuals_in_chunk_n) if i not in set(individuals_train_n)])\n",
    "# #     print(set(individuals_train_n))\n",
    "# #     print(set(individuals_in_chunk_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a1f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "mypath = \"C:/Users/USER/eRisk2017/2017/\"\n",
    "os.chdir(mypath)\n",
    "from os import listdir\n",
    "from glob import glob  # List files\n",
    "\n",
    "# Scrapping\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcb97ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import nltk\n",
    "# Convert to words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# List of stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded418ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_process_train(nChunks, stemm=True, stopword=True, maxlenword=15):\n",
    "    \n",
    "    lengths=[]\n",
    "    # Root Directories\n",
    "    root_p = mypath +'train/positive_examples_anonymous_chunks/chunk '\n",
    "    root_n = mypath +'train/negative_examples_anonymous_chunks/chunk '\n",
    "    \n",
    "    # List of subjects by response type\n",
    "    subjects_p = [subject.replace('_1.xml', '') for subject in listdir(root_p + '1/') if subject != 'desktop.ini']\n",
    "    subjects_n = [subject.replace('_1.xml', '') for subject in listdir(root_n + '1/') if subject != 'desktop.ini']\n",
    "    \n",
    "    # Container DataFrame\n",
    "    # Initially created with only subject IDs and their labels\n",
    "    df_train = pd.DataFrame(subjects_p + subjects_n, columns=['Id'])  # Subject IDs\n",
    "    df_train['Depress'] = len(subjects_p) * [1] + len(subjects_n) * [0]  # Depression label\n",
    "    \n",
    "    # Process each chunk to generate global data\n",
    "    \n",
    "    for chunk in range(1, nChunks + 1):\n",
    "        subjects_p_path = [root_p + str(chunk) + '/' + subject + '_' + str(chunk) + '.xml' for subject in subjects_p]\n",
    "        subjects_n_path = [root_n + str(chunk) + '/' + subject + '_' + str(chunk) + '.xml' for subject in subjects_n]\n",
    "        subjects_train_path = subjects_p_path + subjects_n_path\n",
    "        chunk_posts = []\n",
    "        \n",
    "        for subject in subjects_train_path:\n",
    "            posts_subject = []\n",
    "            infile = open(subject, 'r', encoding='utf8')\n",
    "            post_raw = Soup(infile.read())\n",
    "            posts = post_raw.find_all('text')  # Get all posts\n",
    "            \n",
    "            for post in posts:\n",
    "                if post.getText() != None:\n",
    "                    posts_subject.append(post.getText())\n",
    "                    lengths.append(len(posts_subject))\n",
    "######################################### Preprocessing ############################################################\n",
    "            # 1. Lowercase  2. Stemming 3. Words only        \n",
    "            all_post_subject = ' '.join(posts_subject)\n",
    "            # all_post_subject = re.sub(r'http\\S+', '', all_post_subject)  # Remove web addresses, is.alpha() performs the task\n",
    "            tokens_all_post_subject = word_tokenize(all_post_subject)    # Tokenize\n",
    "            tokens_subject = [token.lower() for token in tokens_all_post_subject]\n",
    "            \n",
    "            if stemm:\n",
    "                if stopword:\n",
    "                    tokens_subject = [ps.stem(token) for token in tokens_subject if token.isalpha() and not token in stop_words and len(token) < maxlenword]\n",
    "                else:\n",
    "                    tokens_subject = [ps.stem(token) for token in tokens_subject if token.isalpha() and len(token) < maxlenword]\n",
    "            else:\n",
    "                if stopword:\n",
    "                    tokens_subject = [token for token in tokens_subject if token.isalpha() and not token in stop_words and len(token) < maxlenword]\n",
    "                else:\n",
    "                    tokens_subject = [token for token in tokens_subject if token.isalpha() and len(token) < maxlenword]\n",
    "            \n",
    "            post_clean = ' '.join(tokens_subject)  # Join all the posts\n",
    "            chunk_posts.append(post_clean)\n",
    "        \n",
    "        name_chunk = 'Chunk_' + str(chunk)\n",
    "        df_train[name_chunk] = chunk_posts\n",
    "   \n",
    "    return df_train, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627bf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_process_test(nChunks, stemm=True, stopword=True, maxlenword=15):\n",
    "    lengths=[]\n",
    "    # Process labels\n",
    "    dir_data_test = \"C:/Users/USER/eRisk2017/2017/test/test_golden_truth.txt\"\n",
    "    y_test = pd.read_csv(dir_data_test, header=None, sep='\\t')\n",
    "#     print (y_test)\n",
    "#     y_test = y_test.drop([0], axis=1)\n",
    "#     print(y_test)\n",
    "    y_test.columns = ['Id', 'Depress']\n",
    "\n",
    "    # Root directory\n",
    "    root_p = mypath+'test/chunk '\n",
    "    # List of subjects by response type\n",
    "    subjects_t = [subject.replace('_1.xml', '') for subject in listdir(root_p+'1/chunk 1/') if subject != 'desktop.ini']\n",
    "#     print(subjects_t)\n",
    "#     subjects_t=subjects_t[0:2]\n",
    "#     print(subjects_t)\n",
    "\n",
    "    # Container DataFrame\n",
    "    df_train = pd.DataFrame(subjects_t, columns=['Id'])  # Subject IDs\n",
    "\n",
    "    # Process each chunk to generate global data\n",
    "    for chunk in range(1, nChunks+1):\n",
    "        subjects_t_path = [root_p + str(chunk) +'/chunk '+ str(chunk) + '/' + subject + '_' + str(chunk) + '.xml' for subject in subjects_t]\n",
    "        chunk_posts = []\n",
    "        for subject in subjects_t_path:\n",
    "            posts_subject = []\n",
    "#             print(subject)\n",
    "            infile = open(subject, 'r', encoding='utf8')\n",
    "            post_raw = Soup(infile.read())\n",
    "#             print(post_raw)\n",
    "            posts = post_raw.find_all('writing')  # Get all posts\n",
    "            for post in posts:\n",
    "                if post.find('title').getText() is not None:\n",
    "                    posts_subject.append(post.find('title').getText())\n",
    "                if post.find('text').getText() is not None:\n",
    "                    posts_subject.append(post.find('text').getText())\n",
    "                lengths.append(len(posts_subject))\n",
    "#             print(posts_subject)\n",
    "            #########################################Preprocessing############################################################\n",
    "            # 1. Lowercase  2. Stemming 3. Words only\n",
    "            all_post_subject = ' '.join(posts_subject)\n",
    "#             print(all_post_subject)\n",
    "            # all_post_subject=re.sub(r'http\\S+', '', all_post_subject)  # Remove web addresses, .isalpha() does the task\n",
    "            tokens_all_post_subject = word_tokenize(all_post_subject)  # Tokenize\n",
    "            tokens_subject = [token.lower() for token in tokens_all_post_subject]\n",
    "#             print(tokens_subject)\n",
    "            if stemm:\n",
    "                if stopword:\n",
    "                    tokens_subject = [ps.stem(token) for token in tokens_subject if\n",
    "                                      token.isalpha() and not token in stop_words and len(token) < maxlenword]\n",
    "                else:\n",
    "                    tokens_subject = [ps.stem(token) for token in tokens_subject if\n",
    "                                      token.isalpha() and len(token) < maxlenword]\n",
    "            else:\n",
    "                if stopword:\n",
    "                    tokens_subject = [token for token in tokens_subject if\n",
    "                                      token.isalpha() and not token in stop_words and len(token) < maxlenword]\n",
    "#                     print(tokens_subject)\n",
    "                else:\n",
    "                    tokens_subject = [token for token in tokens_subject if token.isalpha() and len(token) < maxlenword]\n",
    "            post_clean = ' '.join(tokens_subject)  # Join all posts\n",
    "#             print(post_clean)\n",
    "            chunk_posts.append(post_clean)\n",
    "        name_chunk = 'Chunk_' + str(chunk)\n",
    "#         print(chunk_posts)\n",
    "        df_train[name_chunk] = chunk_posts\n",
    "#     print(y_test['Id'].dtypes)\n",
    "#     print(df_train['Id'].dtypes)\n",
    "    y_test['Id'] = y_test['Id'].str.strip()\n",
    "    df_train['Id'] = df_train['Id'].str.strip()\n",
    "    df_test = pd.merge(y_test, df_train, on='Id', how='inner')\n",
    "    return df_test,lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4421b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,lengths=corpus_process_train(10,stemm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18361f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.18914796473495, 208, 1, 54.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths),np.max(lengths), np.min(lengths), np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26122d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118.88654699603589, 416, 2, 100.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test, lengths =corpus_process_test(10,stemm=False)\n",
    "np.mean(lengths),np.max(lengths), np.min(lengths), np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be8a5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths=[]\n",
    "for i in range(1,11):\n",
    "    ttrain = df_train['Chunk_'+str(i)]\n",
    "    lengths.append(ttrain.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10384705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4990.519547325103, 82324, 0, 1784.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths),np.max(lengths), np.min(lengths), np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11ad3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lengths=[]\n",
    "for i in range(1,11):\n",
    "    ttrain = df_test['Chunk_'+str(i)]\n",
    "    lengths.append(ttrain.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe3a142f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6267.272319201995, 168410, 0, 3194.5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths),np.max(lengths), np.min(lengths), np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77c4133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Depress</th>\n",
       "      <th>Chunk_1</th>\n",
       "      <th>Chunk_2</th>\n",
       "      <th>Chunk_3</th>\n",
       "      <th>Chunk_4</th>\n",
       "      <th>Chunk_5</th>\n",
       "      <th>Chunk_6</th>\n",
       "      <th>Chunk_7</th>\n",
       "      <th>Chunk_8</th>\n",
       "      <th>Chunk_9</th>\n",
       "      <th>Chunk_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_subject1095</td>\n",
       "      <td>1</td>\n",
       "      <td>last line pretty much first could course wasnt...</td>\n",
       "      <td>dammit exact day lykke li kansas city already ...</td>\n",
       "      <td>kinda wish like vertigo lucifer would pulled m...</td>\n",
       "      <td>last night finally decided get slime king summ...</td>\n",
       "      <td>could sworn read somewhere montana highway spe...</td>\n",
       "      <td>get campus shooting guys google play billed ti...</td>\n",
       "      <td>know cinemark chain cheap refills marathon tic...</td>\n",
       "      <td>convergence convergence nightwing oracle conve...</td>\n",
       "      <td>well lightning bolt left bought classic pack g...</td>\n",
       "      <td>believe get next week dark horse neverboy dc c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_subject1190</td>\n",
       "      <td>1</td>\n",
       "      <td>sure get flamed true friends write life choice...</td>\n",
       "      <td>interesting would nice notified people complai...</td>\n",
       "      <td>live close libbie mill really started developi...</td>\n",
       "      <td>way right use zoom lens series primes instead ...</td>\n",
       "      <td>hey live richmond area itching road trip think...</td>\n",
       "      <td>used work police department definitely lightin...</td>\n",
       "      <td>yep hoping kids decent human beings help paren...</td>\n",
       "      <td>point posting seems like kind dick thing hey l...</td>\n",
       "      <td>neighbor sitting backyard last night farting r...</td>\n",
       "      <td>passing offer call ask gold gym posted offer f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Id  Depress  \\\n",
       "0  train_subject1095        1   \n",
       "1  train_subject1190        1   \n",
       "\n",
       "                                             Chunk_1  \\\n",
       "0  last line pretty much first could course wasnt...   \n",
       "1  sure get flamed true friends write life choice...   \n",
       "\n",
       "                                             Chunk_2  \\\n",
       "0  dammit exact day lykke li kansas city already ...   \n",
       "1  interesting would nice notified people complai...   \n",
       "\n",
       "                                             Chunk_3  \\\n",
       "0  kinda wish like vertigo lucifer would pulled m...   \n",
       "1  live close libbie mill really started developi...   \n",
       "\n",
       "                                             Chunk_4  \\\n",
       "0  last night finally decided get slime king summ...   \n",
       "1  way right use zoom lens series primes instead ...   \n",
       "\n",
       "                                             Chunk_5  \\\n",
       "0  could sworn read somewhere montana highway spe...   \n",
       "1  hey live richmond area itching road trip think...   \n",
       "\n",
       "                                             Chunk_6  \\\n",
       "0  get campus shooting guys google play billed ti...   \n",
       "1  used work police department definitely lightin...   \n",
       "\n",
       "                                             Chunk_7  \\\n",
       "0  know cinemark chain cheap refills marathon tic...   \n",
       "1  yep hoping kids decent human beings help paren...   \n",
       "\n",
       "                                             Chunk_8  \\\n",
       "0  convergence convergence nightwing oracle conve...   \n",
       "1  point posting seems like kind dick thing hey l...   \n",
       "\n",
       "                                             Chunk_9  \\\n",
       "0  well lightning bolt left bought classic pack g...   \n",
       "1  neighbor sitting backyard last night farting r...   \n",
       "\n",
       "                                            Chunk_10  \n",
       "0  believe get next week dark horse neverboy dc c...  \n",
       "1  passing offer call ask gold gym posted offer f...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train [0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bc6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Depress</th>\n",
       "      <th>Chunk_1</th>\n",
       "      <th>Chunk_2</th>\n",
       "      <th>Chunk_3</th>\n",
       "      <th>Chunk_4</th>\n",
       "      <th>Chunk_5</th>\n",
       "      <th>Chunk_6</th>\n",
       "      <th>Chunk_7</th>\n",
       "      <th>Chunk_8</th>\n",
       "      <th>Chunk_9</th>\n",
       "      <th>Chunk_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_subject9942</td>\n",
       "      <td>1</td>\n",
       "      <td>know satanic music system based rules works ge...</td>\n",
       "      <td>used problem lane presence get past cc comes p...</td>\n",
       "      <td>jump people insist form counter play lol say e...</td>\n",
       "      <td>transtrenders kind specifically saying nonbina...</td>\n",
       "      <td>gender exists social construct even observable...</td>\n",
       "      <td>messed okay hmm okay helpful thank insight fac...</td>\n",
       "      <td>withdraw class know cases people sure failing ...</td>\n",
       "      <td>pan people want identify bi special snowflake ...</td>\n",
       "      <td>think feminism hurts asexuals namely male asex...</td>\n",
       "      <td>right knowing truth feminism suddenly guys one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_subject3986</td>\n",
       "      <td>1</td>\n",
       "      <td>thanks reminding forgot put nyx eyeliner produ...</td>\n",
       "      <td>wow color looks good recommendation make sure ...</td>\n",
       "      <td>haha highschool senior picture yearbook eyebro...</td>\n",
       "      <td>ah another fellow raccoon eyed eighth grader l...</td>\n",
       "      <td>cosmetics celebration foundation cruelty free ...</td>\n",
       "      <td>valentine day look love leaves scars love blin...</td>\n",
       "      <td>actually returned back japan two months ago li...</td>\n",
       "      <td>years older younger sister born could never re...</td>\n",
       "      <td>trying pursue teaching abroad asia programs re...</td>\n",
       "      <td>guess living small town nothing makes everythi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id  Depress  \\\n",
       "0  test_subject9942        1   \n",
       "1  test_subject3986        1   \n",
       "\n",
       "                                             Chunk_1  \\\n",
       "0  know satanic music system based rules works ge...   \n",
       "1  thanks reminding forgot put nyx eyeliner produ...   \n",
       "\n",
       "                                             Chunk_2  \\\n",
       "0  used problem lane presence get past cc comes p...   \n",
       "1  wow color looks good recommendation make sure ...   \n",
       "\n",
       "                                             Chunk_3  \\\n",
       "0  jump people insist form counter play lol say e...   \n",
       "1  haha highschool senior picture yearbook eyebro...   \n",
       "\n",
       "                                             Chunk_4  \\\n",
       "0  transtrenders kind specifically saying nonbina...   \n",
       "1  ah another fellow raccoon eyed eighth grader l...   \n",
       "\n",
       "                                             Chunk_5  \\\n",
       "0  gender exists social construct even observable...   \n",
       "1  cosmetics celebration foundation cruelty free ...   \n",
       "\n",
       "                                             Chunk_6  \\\n",
       "0  messed okay hmm okay helpful thank insight fac...   \n",
       "1  valentine day look love leaves scars love blin...   \n",
       "\n",
       "                                             Chunk_7  \\\n",
       "0  withdraw class know cases people sure failing ...   \n",
       "1  actually returned back japan two months ago li...   \n",
       "\n",
       "                                             Chunk_8  \\\n",
       "0  pan people want identify bi special snowflake ...   \n",
       "1  years older younger sister born could never re...   \n",
       "\n",
       "                                             Chunk_9  \\\n",
       "0  think feminism hurts asexuals namely male asex...   \n",
       "1  trying pursue teaching abroad asia programs re...   \n",
       "\n",
       "                                            Chunk_10  \n",
       "0  right knowing truth feminism suddenly guys one...  \n",
       "1  guess living small town nothing makes everythi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test [0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66fbb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id          0\n",
       "Depress     0\n",
       "Chunk_1     0\n",
       "Chunk_2     0\n",
       "Chunk_3     0\n",
       "Chunk_4     0\n",
       "Chunk_5     0\n",
       "Chunk_6     0\n",
       "Chunk_7     0\n",
       "Chunk_8     0\n",
       "Chunk_9     0\n",
       "Chunk_10    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30839d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "df_train.to_csv('train_Depression_all_chunks_nosteem.csv', encoding='utf-8', na_rep=' ')\n",
    "df_test.to_csv('test_Depression_all_chunks_nosteem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ec212a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train_Depression_all_chunks_nosteem.csv')\n",
    "train_df=train_df.replace(np.nan, '', regex=True)\n",
    "test_df=pd.read_csv('test_Depression_all_chunks_nosteem.csv')\n",
    "test_df=train_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace879f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "Id            0\n",
       "Depress       0\n",
       "Chunk_1       0\n",
       "Chunk_2       0\n",
       "Chunk_3       0\n",
       "Chunk_4       0\n",
       "Chunk_5       0\n",
       "Chunk_6       0\n",
       "Chunk_7       0\n",
       "Chunk_8       0\n",
       "Chunk_9       0\n",
       "Chunk_10      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47cb4863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "Id            0\n",
       "Depress       0\n",
       "Chunk_1       0\n",
       "Chunk_2       0\n",
       "Chunk_3       0\n",
       "Chunk_4       0\n",
       "Chunk_5       0\n",
       "Chunk_6       0\n",
       "Chunk_7       0\n",
       "Chunk_8       0\n",
       "Chunk_9       0\n",
       "Chunk_10      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63e982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "depression",
   "language": "python",
   "name": "depression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
